---
title: "Blurg: An overlay p2p network for fast block propagation"
date: 2025-04-20T15:34:30-04:00
layout: post
---

![image](https://gist.github.com/user-attachments/assets/265b09ba-0d0f-4924-a71e-3e718e0e8f28)

Idea:
* Transactions (including blobs) are the largest part of a block. If we can speed up TX transmission, we'd speed up block transmission. 
* Assume that you only learn about the TX hashes in block. How do you reconstruct the full TX list as found in a block? You have a good chance of finding transactions in your mempool. You put those into your TX list. But there will be holes in that list because some TX were sent to the builder in private, or simply that your mempool didn't see yet. Luckily, there is a technique that can fix missing pieces in a byte vector almost magically. Erasure coding, like Reed-Solomon, can fill in those missing pieces. We propose to use that in block propagation (This is not a new idea).
* Blurg is a sidecar job to your EL and CL and runs locally on your machine. Blurg should require no changes to CLs or ELs (other than not kicking localhost clients for idleness). 
* Blurg has its own p2p network.
* Blurg connects to your EL using the devp2p port. It does so to receive all the gossip on transactions. It keeps a copy of the mempool this way.
* Blurg connects to your CL using the libp2p interface to listen for new blocks and to feed it new blocks.
* For short-term abuse protection, we can trust privileged pubkeys of community members, relays or builders. For long-term abuse protection, Blurg should only trust the validator pubkey.
* When correctly parametrized (nodes can adjust that dynamically), Blurg is round trip free, yet efficient gossip.
* All you'll find here is a proposal. There is no code for this (yet).

## CL → Blurg → Blurg-P2P

1. Blurg sees its local CL announcing a new block on its libp2p interface. 
2. It fetches the block. 
3. It splits the block into two.
   - `TX`: The original list of transactions (including blobs)
   - `ShortBlock`: A block, in which the list of transaction is replaced with `[(Hash(Tx), Len(Tx))]`
4. Reed-Solomon encode `TX` with a 10x redundancy. This encoding must be deterministic. The result is: `RSedTX := OriginalTxList | RedundancyInformation`, where RedundancyInformation is `9 * len(OriginalTxList)`
5. Chunk up the `RSedTX` in segments of 1000bytes, and prefix them with the indices of the segments. `RSedTXChunks[t] := (t, RSedTX[t*1000:(t+1)*1000])`
6. Distribute the shortened block to all connected peers, and start streaming the redundancy part of `RSedTXChunks` (so the last 90%) including the index of RSedTXChunks to your peers on the blurg-p2p interface. We don't stream the first 10% because we expect peers to have most of that information.

## Blurg-P2P → Blurg → CL

1. Blurg receives `ShortBlock` on its blurg-p2p interface.
2. Blurg prepares an in-memory structure to hold `RSedTXChunks` as follows:
   - Summing all TX lengths inside `ShortBlock` and multiplying that by 10 gives Blurg the size of `RSedTXChunks`. Initialize everything with zero.
   - Fill the start of the first 10% of `RSedTXChunks` with transactions that were already public. Remember Blurg tees the mempool from the ELs devp2p interface. Blurg grabs full transactions from the mempool (indexed by txhash of course) and places each transaction into the right spot as indicated by `[(Hash(TX), Len(TX))]` in `ShortBlock`. 
3. After receiving the shortened block, the remote Blurg streams chunks from `RSedTXChunk`. Note that there can be many peers streaming at once.
4. Blurg checks the authentication information on the chunk (see below for a design proposal). If the check fails, discard the chunk.
5. Blurg places the received chunk in the apprioriate place into `RSedTXChunks` as indicated by the received index. 
6. Blurg then forwards the chunks to its peers (after apply appropriate flow-controls).
7. Blurg roughly knows when it has received enough chunks to attempt to RS decode everything.
9. If Blurg succeeds with RS decoding `TX`, it sends a FeedUp message to peers to stop them from streaming. Otherwise wait for more chunks
10. Blurg now holds the full original `TX`. `TX` is found in the first 10% of the `RSedTXChunks` array it has decoded. Blurg reconstructs the long block from `ShortBlock` by substituting the hash of a transaction for the whole transaction. It hands the block to the CL.
11. Now here is the nice part: Blurg can recover the full `RSedTXChunk` array by redoing the deterministic RS encoding on the original TX list, which it now holds. This allows it to start streaming chunks to its peers from RSedTXChunks that it has never received. This helps to reset the chance of duplicate chunks streaming (see below). 

## Flow-control

Instead of indiscriminately pushing all RSedTXChunks to remote peers, the two peers could agree on a number of chunks to be
streamed as initial flush, after which the receiving node needs to request more. The receiving node can adjust this number
over time based on its experience. If it didn't receive enough chunks on the initial flush without having to ask for more (+1RTT)
it could increase the number. If it receives way too much, it asks its peers to lower the initial stream of chunks.

## Abuse/Flooding protection

It is necessary that Blurgs can authenticate the chunks in some way before forwarding them to peers. Otherwise, a peer might 
send an invalid chunk, we forward that, and at the end, we end up banned, because the remote peer, that we 
optimistically forwarded the chunk to, can't attribute the violation cleanly and blames us.

Before we discuss any flood protection, note that we can get away with much less secure primitives. For commitment 
schemes involving hashes and signatures, we don't require anywhere near 256-bit collision resistance. Our primitives 
need to survive brute-forcing of maybe for 4 seconds. We are absolutely fine with using hashes with a 32-bit security margin. Even if once in awhile an attacker manages to brute force some element of the proof chain, the harm is very limited. Nothing fails catastrophically with Blurg. Having broken RSedTXChunk is not harmful in our RS decoding. The attacker is just wasting our bandwidth. We only need to make sure that we don't become an attack amplifier, and authenticated chunks solve that.

### Signed merkle roots

One option is for the block producer to sign a merkle root with their BLS public key. We stick that signature and the 
merkle root into `ShortBlock` (some optimizations on that later). Remember, `ShortBlock` is relayed to all peers as first message.

Note that the first peer propagating the block knows the full block, therefore knows the full TX list, therefore knows `RSedTXChunks` in its entirety and therefore can produce any merkle proof they want leading up to the signed merkle root by the producer. When nodes receive chunks + proof, they verify the proof and only then forward the chunk including the proof. Once they have received enough chunks, they can recover TX and then recover `RSedTXChunks` in its entirety. Once they have all of RSedTXChunks, they can generate proofs for chunks that they never received but recovered.

**Napkin math**: Let's assume that an Ethereum block is [200kb](https://etherscan.io/chart/blocksize). That requires 200 chunks á 1000 bytes. Blowing that up by 10x gives us 2000 chunks. That requires a merkle tree with 11 layers, because k=11 is the smallest k for which (2000<2^k). With a 32-bit hash, we have an overhead of 4 * 11 bytes in proof data per chunk. That makes the chunk 1044 bytes, which is a 4.4% overhead for making chunks authenticated.

We can optimize this napkin math a bit. Let's say we stick all nodes in the fifth layer of the merkle tree into `ShortBlock`. 
You can still recompute the merkle root from only that layer. That would lead to a 128 byte increase (=2^5 * 4 bytes) of `ShortBlock`, but it would allow us to shorten the merkle proofs in the chunks by 5 layers. 
With 6 layers remaining to be proven for each chunk, we have reduced our overhead per chunk from 44 bytes to 24 bytes. That's a pretty good trade-off. (FIXME: Do some math on how many chunks we expect on average, and then compute the layer number that should be sent in `ShortBlock`. Maybe sending an even lower layer is better.)

### Vector commits

FIXME: Figure out whether we can do everything with a vector commit. It probably depends on how fast the primitives are. Again consider using 32-bit reduced security. If necessary, we could have the signer commit via its strong BLS pubkey to a weaker pubkey, let's say on a 32-bit curve, which we then use for vector commits. Again, we only have to survive a few seconds of brute forcing with this structure. Once nodes have received enough chunks to recover the full TX list, we don't need any more proofs.

### Who generates signed commitments?

Options:
* Best case: The validator for the current slots signs the commitment on `RSedTXChunk` itself. In partice, we wouldn't want to burden the VC with that, but have Blurg go behind the back of the VC and request a BLS-signature from Dirk/Web3signer on a string that's always safe to sign, e.g. "Blurg" | Slot | MerkleRoot. Blurgs on the receiving end of chunks could query their local CL a couple of seconds before the slot starts to figure out what's the validator pubkey for the next slot. I would not making signing the wrong commitment a slashable offense by the validator, since there is no incentive misalignment: The validator wants to sign the right commitment because it wants to support fast block propagation.
<!-- * Okay case: Co-opt block builders. Block builders know their TX list. Therefore they can compute RSedTXChunk, arrange everything in a merkle tree and sign the merkle root. --> 
* Okay case: Geo-distribute Blurgs around the globe run by community members. Each of them can produce a `ShortBlock`, `TX`, `RSedTXChunk` and the associated signed commitment on the latter. Receiving Blurgs check the signature against a list of "known-to-be-good" pubkeys.

### Where to put actually signatures?

If we relay on anyone else than validators for commitments, we need to specify which builders or community members we trust. When connecting to another Blurg, our instance could signal to the remote instance, what specific pubkeys we trust. The remote instance doesnt' forward chunks until it sees a commitment from one of our trusted signatures. If a block comes in with a trusted signature, the remote Blurg prepares a `ShortBlock` with a trusted commitment.

How to actually get data into Blurg? Some unfinished ideas:
* If we have a local Blurg next to a validator, we could have Blurg produce a signature on the commitment directly by talking to a signing backend like Dirk or Web3Signer.
* Alternatively, validators could stick their signatures into the block they produce. There aren't many places to stick additional data that isn't subject to some validation rules. The graffiti field is one field without validation requirements. Blurg could try to extract a signature from the graffiti. The signed message is implicit from the rest of the block.
* If we get block builders involved, we could use the last transaction in the produced block to commit to all previous transactions. We need to exclude the last TX from `RSedTXChunk`, because otherwise the commitment is inside the structure that commits to.

<!-- 
* If we relay on out-of-Ethereum-p2p-band commitments (generated by community member Blurgs), 

Random thought: If we don't want to relay on the Blurgs of the builders to propagate `ShortBlock` with the signature on the proof, we could sneak the signature on the merkle-root on `RSedTXChunk` into the last transaction generated by in a block. Most of the time the last transaction is generated by the builder anyway, because they usually send the MEV payout this way to the fee recipient. While this transaction is usually a simple send, I am sure there is away to stick some extra data in there. Once we have the last TX include the signature, then all the plumbing that exists for blocks is suddenly available for us to use as signature plumbing.

Readers might have noticed that now I have a recursion problem because the signature is part of the data that it authenticates, which is recursive and uncomputable. The trick to fix this is to remove the last transaction from the TX list that we blow up into `RSedTXChunk`. Having the last transaction in there is pretty much useless anyway, because as said the last transaction is generated by the builder, so it is never part of the mempool, so all nodes need to learn it anyway. We could just stick the last tx into `ShortBlock` verbatim. That removes the recursion problem, if we ask builders from this extra signature inside their payout TX.

FIXME: Edit that the following into the above. The above works probably much better if we don't explicitly trust the builders but trust the relays. In an MEV setup, we
already need to trust their keys. Maybe they could be the ones generating the signature on the merkle root on the bloated RSedTXChunks.  -->

## FAQ

> UDP?

I think that picking UDP for chunk streaming is the right choice. We don't need reliable transmission. The chunk size + overhead (1024 bytes) should fit into the normal MTU.

> Effect on the network?

By using the mempool to bootstrap part of `RSedTXChunk`, we effectively only need to propagate the private transactions included
by builders. I expect a significant decrease in bandwidth required for block propagation. If we assume that out of a 
200kb Ethereum block we have around 5% private transactions, we need to transmit 10kb of erasure coding information, 
which equates to 10 UDP packets.

> Why such a huge 10x blow-up? 
 
We would like to minimize the chance that two independent peers stream the same chunk to a peer. If those two peers pick random parts for `RSedTXChunks` its less likely that they stream identical chunks if `RSedTXChunks` is large.

> Solana's Turbine? 

Turbine also uses Reed-Solomon, so that's similar. But Turbine also does stake-weighted propagation, meaning large stakers get chunks first. I believe that this is a centralizing force, and I don't want to support that.

> Deployment?

Deploying Blurg is entirely optional, but I do assume that professional node operators would want to start interconnect this way, especially if block builders join that effort. It's rational to join blurg-p2p as everyone benefits from fast block propagation. If there are any block withholding games that could be played, I would argue that they are less effective with Blurg, as every peers could fill in for each other on the `RSedTXChunk` level, which is much cheaper for them to do, because chunks are much smaller than blocks.
 
> Bitcoin?

The approach should be generic and it should work with other blockchains with public mempools. 

> Long-Term?

If Blurg delivers on its promises, we could make validators BLS-sign the merkle roots and incorporate the overlay p2p network into the CL p2p network directly.

> Centralization risks?

If we get validators to sign the merkle roots, Ethereum isn't any less centralized. However, if we have to permanently relay on trusting builder keys or community member keys as discussed in the flood protection section and if we ratched up 
blob count and block sizes so much that block propagation wouldn't work anymore without Blurg, then we have affectively harmed Ethereum's decentralization. So let's not argue any block size/blob count increases until validators sign proofs with their own keys.

> Speed?

I want Blurg to be able to start streaming chunks 50ms after it received the full block from the CL. For that it requires: `ShortBlock`, `RSedTXChunk` and a signature on the merkle root implied from all RSedTXChunk. Pardon me using ChatGPT references. I didn't have time to do all those benchmarks by hand. But here is the napkin math:

1. Rearranging the long block into a ShortBlock is trivial. You just Keccak256-hash the <400 transactions. That's 0.4ms according to ChatGPT. We could also use a different hash algorithm here. That's a Blurg internal choice.
2. We need to RS encode everything. I do think that we have existing RS implementations that are fast enough. Quote from the [FastECC](https://github.com/Bulat-Ziganshin/FastECC) Github repo, which is a bit old: "FastECC is open-source library implementing O(N * log(N)) encoding algorithm. It computes million parity blocks at 1.2 GB/s." We have <2000 parity blocks. That seems to be a trivial amount. The FastECC repo talks about much faster libraries. It also suggests looking into LDPCs. ChatGPT says 0.04ms, but I can't confirm that because that's not my area of expertise. Again relaying on my friend ChatGPT, we seem to land somewhere in the 0.01ms range to encode 200 1kb blocks with 1800 parity blocks.
3. We need to produce a merkle-root on a structure with 2000 leaves. That's around 4000 32-bit hashes. If we just take BLAKE3 and cut off the first 32-bits as result, we'd still only require 0.9ms to get 4000 hashes done. (Says ChatGPT) 
4. We need some kind of signature on the merkle root. If we use validator keys to sign, we probably want a simple BLS signature. I also believe that's trivial.

So if ChatGPT is to be believed with all those timing calculations, Blurg could easily observe a 50ms deadline. Even 10ms seem entirely feasible to finish all the internal work.

> Why a side car?

I think that this is just a much easier way to get started to prove the concept. It sidesteps all discussion on how to change the libp2p layer, and allows me to make progress fast. It also allows me to fail without consequences and creating technical debt. 

> p2p bootstrapping?

We can bootstrap off of a set of semi-decentralized DNS endpoints that round-robin emit nodes on the p2p network on A record lookup. Other less centralized alternatives exist.

> Can we support non-public transactions?

If builders would support this effort fully, we could have Blurg establish an encrypted mempool. For each TX sent to the builder's private TX API, they could generate a symmetric key, hash the key, and publish a tuple of `(H(K),E_K(ConfidentialTXData))` to the encrypted mempool maintained by Blurg via its p2p network. This should happen as soon as TXs are sent to the private API, so their encrypted copy has time to propagate, probably a few seconds before the block must be built at slot time. Receiving Blurgs hold a mempool indexed from `H(K) -> E_K(ConfidentialTXData)`, of course both objects are opaque to the receiving Blurgs, because they don't have K, so it's essentially a `bytevector -> bytevector` lookup table.

When a TX is recruited into the block, instead of publishing `(Hash(TX),Len(TX))` in `ShortBlock`, the builder
could publish `(K,Len(ConfidentialTXData))`. The receiving Blurg computes `H(K)` from K, and looks up whether there is an encrypted mempool element. If yes, use K to decrypt `ConfidentialTXData`. `ConfidentialTXData` is pasted into the correct spot in the initial 10% of `RSedTXChunks` just as an unencrypted TX would have been pasted. If the process fails for some reason, we'd have to wait for more `RSedTXChunks` to stream in to recover the unencrypted TX data of the confidential TX.

A moderate complication arises from the fact that now we have potentially two `ShortBlock` annoucements travelling around on the p2p network. There are the ones generated by nodes that don't know the builder keys and that will just have `(Hash(TX),Len(TX))` in their TX list. That's not going to work, as the transaction isn't in the public mempool. And then there is the `ShortBlock` from the builder having `(K,Len(TX))`. We probably have to switch to resending `ShortBlock` to our peers, in case the one from the builder comes in.

Nit: Now that we have `(Hash(TX), Len(TX))` and `(K,Len(TX)` mixed in the TX list inside `ShortBlock`, we need a bit to differentiate those two object types. Or we just try both types and see what succeeds, would also work.

Nit: [Flood protection?](https://gist.github.com/catwith1hat/0e8b8869bca23dba4a95548343c12a1c)

> Credits!

This idea isn't new. Inspirations drawn from discussions with sipa, IDA-Gossip, Turbine. I think that the reduced security argument for flood protection is new.

> Next steps?

Feeling cute, might implement later.
